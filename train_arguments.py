### Train arguments for train_model.py and run_train_model.sh
import argparse

def parse_args():
    parser = argparse.ArgumentParser(description="CLEF 2025 5th Shared Task 'EXIST - sEXism Identification in Social neTworks' - Subtask 1.3 Sexism Categorization")
    # str
    parser.add_argument("--wandb_project", type=str, required=True, help="Wandb project title")
    parser.add_argument("--wandb_name", type=str, required=True, help="Wandb project name")
    parser.add_argument("--pretrained_model", type=str, required=True, help="Pre-trained model name (HuggingFace or local)")
    parser.add_argument("--train_dataset_path", type=str, required=True, help="Training dataset path")
    parser.add_argument("--dev_dataset_path", type=str, required=True, help="Development dataset path")
    parser.add_argument("--test_dataset_path", type=str, required=True, help="Test dataset path")
    parser.add_argument("--output_dir", type=str, required=True, help="Output directory")
    parser.add_argument("--text_column", type=str, default="tweet", required=True, help="Column name for text data")
    parser.add_argument("--sentiment_column", type=str, default="sentiment", required=True, help="Column name for sentment data")
    parser.add_argument("--label_column", type=str, choices=["soft_labels", "hard_labels"], required=True, default="hard_label", help="Column name for label data")
    parser.add_argument("--evaluation_type", type=str, choices=["soft", "hard"], default="hard", help="Select for soft or hard label evaluation")
    parser.add_argument("--val_predictions_file_name", type=str, required=True, help="Validation predictions tsv filename")
    parser.add_argument("--dev_predictions_file_name", type=str, required=True, help="Development predictions tsv filename")
    parser.add_argument("--test_predictions_file_name", type=str, required=True, help="Test predictions tsv filename")
    parser.add_argument("--problem_type", default="multi_label_classification", choices={"multi_label_classification", "single_label_classification"}, type=str, required=True, help="Classification problem type")
    parser.add_argument("--language", type=str, required=True, choices={"all","en","es"}, help="Selected language")
    parser.add_argument("--pooling_method", type=str, required=True, choices={"cls","mean","max"}, help="Selected pooling method for multi-task model")
    parser.add_argument("--report_to", default="wandb", choices={"wandb", "tensorboard"}, type=str, required=True, help="Platform to report the results")
    parser.add_argument("--evaluation_strategy", default="epoch", choices={"no", "epoch", "steps"}, type=str, required=True, help="Evaluation training strategy")
    parser.add_argument("--logging_strategy", default="epoch", choices={"no", "epoch", "steps"}, type=str, required=True, help="Logging training strategy")
    parser.add_argument("--save_strategy", default="epoch", choices={"no", "epoch", "steps"}, type=str, required=True, help="Checkpoint save training strategy")
    parser.add_argument("--lr_scheduler_type", default="linear", choices={"linear", "cosine"}, type=str, required=True, help="Learning scheduler type")
    parser.add_argument("--optimizer",  default="adamw_torch_fused", type=str, required=True, help="Optimizer name")
    parser.add_argument("--hub_strategy", default="every_save", choices={"end", "every_save", "checkpoint", "all_checkpoints"}, type=str, required=True, help="Scope of what is pushed to the Hub and when")
    parser.add_argument("--metric_for_best_model", default="macro-avg-f1-score",  type=str, required=True, help="Selected model metric") 
    parser.add_argument("--padding", type=str, default="max_length", required=True, choices={"longest", "max_length", "do_not_pad"}, help="Padding method")
    parser.add_argument("--padding_side", type=str, required=False, choices={"left", "right"}, help="Padding side")
    parser.add_argument("--truncation", type=str, default="longest_first", required=True, choices={"longest_first", "only_second", "only_first", "do_not_truncate"}, help="Padding method")
    parser.add_argument("--loss_function", type=str, required=True, choices={"FL", "CBloss", "DBloss", "CBloss-ntr", "BCE"}, help="Loss Function")
    parser.add_argument("--model_architecture", type=str, required=True, choices={"multi-task", "baseline"}, help="Model architecture")

    # int or float
    parser.add_argument("--num_labels", type=int, required=True, help="Number of labels")
    parser.add_argument("--seed", type=int, required=True, help="Seed for results reproducibility")
    parser.add_argument("--max_seq_length", type=int, required=True, help="Maximum length of the input sequences")
    parser.add_argument("--train_batch_size", type=int, required=True, help="Train batch size")
    parser.add_argument("--val_batch_size", type=int, required=True, help="Validation & test batch size")
    parser.add_argument("--num_train_epochs", type=int, required=True, help="Number of training epochs")
    parser.add_argument("--warmup_steps", type=int, required=False, help="Number of warm-up steps")
    parser.add_argument("--gradient_accumulation_steps", type=int, required=False, help="Number of gradient accumulation steps")
    parser.add_argument("--early_stopping_patience", type=int, required=True, help="Early stopping patience")
    parser.add_argument("--save_steps", type=int, required=True, help="Number of save steps")
    parser.add_argument("--logging_steps", type=int, required=True, help="Number of logging steps")
    parser.add_argument("--eval_steps", type=int, required=True, help="Number of evaluation steps")
    parser.add_argument("--save_total_limit", type=int, required=True, help="Number of saved checkpoints")
    parser.add_argument("--learning_rate", type=float, required=True, help="Learning rate")
    parser.add_argument("--weight_decay", type=float, required=True, help="Weight decay")
    parser.add_argument("--warmup_ratio", type=float, required=True, help="Warm-up ratio")
    parser.add_argument("--adam_epsilon", type=float, required=True, help="Adam epsilon in optimizer")
    parser.add_argument("--data_split_ratio", type=float, required=True, help="Data split ratio")

    # boolean
    parser.add_argument("--curriculum_learning", default=False, required=False,  help="Select to do curriculum learning")
    parser.add_argument("--add_special_tokens_context", default=True, required=False,  help="Add special tokens for context")
    parser.add_argument("--add_special_tokens", default=True, required=False, help="Add special tokens")
    parser.add_argument("--return_attention_mask", default=True, required=False,  help="Return attention mask")
    parser.add_argument("--set_pad_id", default=True, required=False, help="Set the id for the padding token")
    parser.add_argument("--use_fast", default=True, required=False, help="Use fast model tokenizer")
    parser.add_argument("--do_lower_case", default=False, required=False, help="Lowercase tokens")
    parser.add_argument("--gradient_checkpointing", default=True, required=False, help="Use gradient checkpointing to save memory at the expense of slower backward pass")
    parser.add_argument("--group_by_length", default=False, required=False, help="Group together samples of roughly the same length in the training dataset to minimize padding applied and be more efficient")
    parser.add_argument("--greater_is_better", default=True, required=False, help="If better models should have a greater metric or not")
    parser.add_argument("--load_best_model_at_end", default=True, required=False, help="Load best model at end")
    parser.add_argument("--overwrite_output_dir", default=True, required=False, help="Ovewrite content of output directory")
    parser.add_argument("--push_to_hub", default=False, required=False, help="Push to Hugging Face")
    parser.add_argument("--fp16", default=True, required=False, help="Use fp16 16-bit (mixed) precision training instead of 32-bit training")
    parser.add_argument("--fp16_full_eval", default=True, required=False, help="Use fp16 16-bit (mixed) precision validation")
    parser.add_argument("--bf16", default=False, required=False, help="Use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher NVIDIA architecture or using CPU (use_cpu) or Ascend NPU")

    # PEFT (Parameter Efficient Tuning)
    parser.add_argument("--peft_type", type=str, choices={"None", "PrefixTuning", "PromptTuning", "LoRA", "PromptEncoder"}, required=True, help="PEFT method to use")
    parser.add_argument("--task_type", type=str, default="SEQ_CLS", required=False, choices={"SEQ_CLS", "CAUSAL_LM", "SEQ_2_SEQ_LM", "TOKEN_CLS"}, help="Type of task to perform")
    parser.add_argument("--lora_bias", type=str, default="none", required=False, choices={"lora_only", "none", "all"}, help="Layers to add learnable bias")
    parser.add_argument("--inference_mode", action="store_true", required=False, help="Use the Peft model in inference mode")
    parser.add_argument("--num_virtual_tokens", type=int, default=50, required=False, help="Virtual tokens for PromptTuning and PromptEncoder")
    parser.add_argument("--lora_r", type=int, default=16, required=False, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=32, required=False, help="LoRA alpha")
    parser.add_argument("--lora_dropout", type=float, default=0.1, required=False, help="LoRA dropout")

    arguments = parser.parse_args()
    return arguments


if __name__ == "__main__":
    args = parse_args()
